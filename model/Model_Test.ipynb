{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Crossme0809/frenzyTechAI/blob/main/fine-tuned-llm-trainer/How_to_Fine_Tune_and_Train_LLMs_With_FAST_GPT_LLM_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbrFgrhG_xYi"
   },
   "source": [
    "# 安装必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 09:08:18.698686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7\"\n",
    "from model.llama import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lPG7wEPetFx2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.40.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n",
    "import transformers\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\" #指定cuda可见显卡编号\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6fux9om_c4-"
   },
   "source": [
    "#运行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi #查看显卡占用情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd5f71168b4335bdeeb8ea04f4ffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#加载模型\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model_path = \"/pubshare/LLM/Llama-2-70b-chat-hf\"  # 更改为保存模型的路径,可用范围：MineLLaMa、MineLLaMa-v3、MineLLaMa-v4\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map='auto')\n",
    "                                            # device_map='auto')  #device_map={\"\": 5}或device_map={\"\": 6}或device_map={\"\": 7}，使用前可调用上代码框中的!nvidia-smi查看显卡占用情况\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#推理\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                max_length=1024,\n",
    "                # repetition_penalty=1.18,\n",
    "                # no_repeat_ngram_size=5,\n",
    "                return_full_text=False,\n",
    "                temperature=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [\n",
      "\"craft wooden sword\",\n",
      "\"craft wooden helmet\",\n",
      "\"craft wooden chestplate\",\n",
      "\"craft wooden leggings\",\n",
      "\"craft wooden boots\",\n",
      "\"kill zombie\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "user_prompt='Task list from last round: None; Critique: None; Monster: 1 zombie.'\n",
    "system_prompt='''\n",
    "In Minecraft, combating with monsters requires weapons and armor.\\nThe weapon options are limited to \"sword\", while the armor includes \"helmet\", \"chestplate\", \"leggings\", and \"boots\".\\nThe materials for swords range from low to high level: wooden swords, stone swords, iron swords, and diamond swords;\\nThe materials for armor range from low to high level: iron, diamond.\\nThe higher the material level, the greater the attack damage of the weapon and the better the protection effect of the armor.\\nHowever, the higher the material level, the more time it costs to collect.\\n\\nTips: Wooden, stone, iron and diamond are the only levels of sword; iron and diamond are the only levels of armors;\\nhelmet, chestplate, leggings and boots are the only types of armors; do not generate information that doesn\\'t relate to them.\\n\\nYou goal is to generate the plan that can defeat all monsters while using the shortest time. After each round of combat, I will give you:\\n\\nTask list from last round: ...\\nCritique: ...\\nMonster: The monsters you need to defeat.\\n\\nThe critique (if any) will tell you the subgoal list from the previous round \n",
    "and whether you should trim or add to it.\\nRemember to refer to the critique to adjust your task list.\\n\\nYou must follow the following criteria:\\n1) Return a Python list of subgoals that can be completed in order to complete the specified task.\\n2) Each subgoal should start with \"craft\", do not return any other type of skills.\\n3) Each subgoal should follow a concise format \"craft [material type] [equipment type]\" such as \"craft stone sword\" and \"craft iron helmet\".\\n\\nYou should only respond in JSON format as described below:\\n[\"subgoal1\", \"subgoal2\", \"subgoal3\", ...]\\nEnsure the response can be parsed by Python `json.loads`, e.g.: no trailing commas, no single quotes, etc.\\nThe [] format will be used for RE extraction, do not use this format for anything other than your plan list and no extrabrackets!\n",
    "'''\n",
    "\n",
    "input_text = f'[INST]<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n {user_prompt}\\n[/INST]' \n",
    "result = pipe(input_text)\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
